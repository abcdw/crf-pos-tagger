* Introduction
The goal of the project is to implement part-of-speech([[https://en.wikipedia.org/wiki/Part-of-speech_tagging][pos)]] tagger using
[[https://pystruct.github.io/intro.html][structured learning]].

This project uses [[http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/][Conditional Random Fields]] modeling method
and [[https://media.readthedocs.org/pdf/sklearn-crfsuite/latest/sklearn-crfsuite.pdf][sklearn-crfsuite]] implementation of this method.

Source code of the project can be found on [[https://github.com/abcdw/crf-pos-tagger][github]].

* Baseline
Basic implementation of the tagger is similar to sklearn-crfsuite [[http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html][tutorial]], but
uses different data sets, feature sets and labels.

** Datasets
~crf-pos-tagger~ uses ~pos_train.conll~ dataset for training model and
~pos_test.conll~ for evaluating results.

Datasets contains twitter messages, which contains pairs (pos tag, token) and
separated by empty line. Token is a word, mention, url, hashtag, number,
punctuation mark, special symbol and so on. Twits are not very good in terms of
right spelling(ppl, u, ill, etc) and absolutely afwul in terms of cases(i LOVE U
sO MuCH). Also, it is pretty hard to separate sentences, it may finishes with
period of may not. That is why ~crf-pos-tagger~ uses twits instead of sentences.

** Parser
For such dataset and with assumption about ~twit ~== sentence~ it is pretty easy
to write parser, which generates list of lists of pairs.

#+BEGIN_SRC python
def parse_file(filename):
    f = open(filename, 'r')
    raw = f.readlines()
    sentences = []
    s = []
    for line in raw:
        if line.strip():
            tag, token = line.strip().split('\t')
            s.append((token, tag))
        else:
            sentences.append(s)
            s = []
    return sentences
#+END_SRC

** Algorithms
~sklearn-crfsuite~ provides five algorithms:
– 'lbfgs' - Gradient descent using the L-BFGS method
– 'l2sgd' - Stochastic Gradient Descent with L2 regularization term
– 'ap' - Averaged Perceptron
– 'pa' - Passive Aggressive (PA)
– 'arow' - Adaptive Regularization Of Weight Vector (AROW)

Choice of algorithm wasn't based on implementation details. Playing with
different values of different parameters and different feature sets showed that
following configuration one of the best results for project needs:

#+BEGIN_SRC python
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
)
#+END_SRC

** Evaluation
Evaluation is straightforward: ~crf-pos-tagger~ compares tags generated by model
and tags written in ~pos_test.conll~ for each token and calculates number of
matched divided by number of all tokens.

Default implementation without features shows the percentage of matches equal to
~0.1053~.

* Improvements

|------------------+--------|
| feature set      | result |
|------------------+--------|
| without features | 0.1053 |
|                  |        |
